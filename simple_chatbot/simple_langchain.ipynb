{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install the required packages\n"
      ],
      "metadata": {
        "id": "3bsl23CqjS-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.33.2 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off"
      ],
      "metadata": {
        "id": "TK5RHFW8cjw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Tokenizer: The tokenizer is loaded from the pre-trained TheBloke/Llama-2-13b-Chat-GPTQ model, enabling text input to be converted into tokenized format for processing.\n",
        "\n",
        "Load the Model: The GPTQ (quantized) version of Llama-2-13B is loaded, configured to run efficiently on compatible hardware with torch.float16 and automatic device mapping.\n",
        "\n",
        "Set Generation Parameters: The GenerationConfig defines output behavior, including the number of tokens, randomness (temperature), sampling, and penalties for repetitive outputs.\n",
        "\n",
        "Pipeline and LangChain Integration: A text generation pipeline is created and wrapped into a LangChain-compatible HuggingFacePipeline for easy integration into language model workflows."
      ],
      "metadata": {
        "id": "r9XFRN8QlUeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "Hrb-NGx4eOJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(\n",
        "    \" What is the capital City of Ethiopia\"\n",
        ")\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR-cRzq8hm5U",
        "outputId": "279aa191-98e5-4d91-df81-6295d5638c8e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?\n",
            "\n",
            "Answer: The capital city of Ethiopia is Addis Ababa.\n"
          ]
        }
      ]
    }
  ]
}